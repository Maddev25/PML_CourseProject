View(dfbeta(fit))
resno <- out2[1,"y"] - predict(fitno, out2[1,])
1- resid(fit)[1]/resno
head(hatvalues(fit))
sigma <- sd(fit)$resid / deviance(fit)
sigma <- fit$resid
sigma <- sqrt(deviance(fit)/df.residual(fit))
rstd <- sigma * sqrt(1-hatvalues(fit))
rstd <- resid(fit) / sigma * sqrt(1-hatvalues(fit))
rstd <- resid(fit) / (sigma * sqrt(1-hatvalues(fit)))
head(cbind(rstd,rstandard(fit)))
plot(fit, which = 3)
plot(fit, which = 2)
sigma1 <- resid(fitno)/sqrt(df(fitno))
sigma1 <- resid(fitno)/sqrt(deviance(fitno))
sigma1 <- sqrt(deviance(fitno)/df.residual(fitno))
resid(fit)[1]/ (sigma1 * sqrt(1-hatvalues(fit)[1])
)
head(rstudent(fit))
dy<- predict(fitno, out2) - predict(fit,out2)
dy/2*sigma^2
sum(dy^2)/(2*sigma^2)
plot(fit, which = 5)
swirl()
q()
data(mtcars)
fit <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
summary(fit)$coef
fitnew <- lm (mpg ~ factor(cyl), data = mtcars)
summary(fitnew)$coef
fitinter <- lm (mpg ~ factor(cyl) * wt, data=mtcars)
annova(fit,fitinter, test="Chisq" )
library(car)
install.packages(car)
library(stats)
annova(fit,fitinter, test="Chisq" )
anova(fit,fitinter, test="Chisq" )
fit4 <- lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)
summary(fit4)$coef
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit6 <- lm(y~x)
hatvalues(fit6)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit7 <- lm(y~x)
dfbeta(fit7)
dfbetas(fit7)
dfbetas(fit7)[,2]
q()
cd\
library(swirl)
swirl()
install.packages("Car")
install.packages("car")
library(car)
swirl()
remove.packages("car")
remove.packages("minga")
remove.packages("nloptr")
remove.packages("RcppEigen")
remove.packages("lme4")
remove.packages("SparseM")
remove.packages("MatrixModels")
remove.packages("pbkrtest")
remove.packages("quantreg")
q()
install.packages("car")
library(car)
install.packages("pbkrtest")
install.packages("nloptr")
remove.packages("nloptr")
install.packages("nloptr")
library(car)
swirl()
library(swirl)
rm(list = ls())
q()
library(swirl)
swirl()
rgp1()
rgp2()
head(swiss)
mdl <- lm(Fertility ~ ., swiss)
vif(mdl)
mdl2 <- lm(Fertility ~ . - Examination, swiss)
vif(mdl2)
x1c<- simbias()
apply(x1c,1, mean)
fit1 <- lm(Fertility ~ Agriculture, swiss)
fit3 <- lm(Fertility ~ Agriculture + Examination + Education, swiss)
anova(fit1, fit3)
deviance(fit3)
d <- deviance(fit3)/43
n <- deviance(fit1) - deviance(fit3) / 2
n <- (deviance(fit1) - deviance(fit3)) / 2
n/d
pf(n/d, 2, 43, lower.tail = FALSE)
shapiro.test(fit3$residuals)
anova(fit1, fit3, fit5, fit6)
q()
library(swirl)
swirl()
bye
q()
library(swirl)
swirl()
ravenData
mdl <- glm(ravenWinNum ~ ravenScore, family = binomial, ravenData)
lodds <- predict(mdl, data.frame(ravenScore = c(0,3,6)))
exp(lodds)/(1 + exp(lodds))
summary(mdl)
confint(mdl)
exp(confint(mdl))
anova(mdl)
qchisq(0.95,1)
var(rpois(1000,50))
head(hits)
class(hits[,'date'])
as.integer(head(hits[,'date']))
mdl <- glm(visits ~ date, poisson, hits)
summary(mdl)
exp(confint(mdl, 'date'))
which.max(hits[,'visits'])
hits[704,]
lambda <- mdl$fitted.values[704]
qpois(0.95, lambda)
mdl2 <- glm(visits ~ date , poisson,offset = log(visits+1), hits)
mdl2 <- glm(simplystats ~ date , family = poisson,offset = log(visits+1), data =hits)
qpois(0.95,mdl2$fitted.values[704] )
q()
library(swirl)
swirl()
library(caret)
install.packages("caret")
library(caret)
install.packages("iterators")
remove.packages(iterators)
install.packages("iterators")
library(caret)
remove.packages(broom)
install.packages(broom)
install.packages("broom")
library(caret)
install.packages("dimRed")
library(caret)
install.packages("CVST")
library(caret)
install.packages("prodlim")
install.packages("prodlim")
library(caret)
library(caret)
q()
library(caret)
library(AppliedPredictiveModeling)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
training
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
names <-colnames(concrete)
names <- names[-length(names)]
names
names <-colnames(concrete)
names
length(names)
names <- names[-9]
names
featurePlot(x = training[,names], y= training$CompressiveStrength, plot = "pairs")
q()
library(caret)
library(kernlab)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
lilbrary(Hmisc)
library(Hmisc)
install.packages("Hmisc")
install.packages("checkmate")
adData <- data.frame(diagnosis, predictors)
adData
head(adData)
dim(adData)
train <- createDataPartition(diagnosis, p=0.5, list = FALSE)
test <- createDataPartition(diagnosis, p=0.5, list = FALSE)
testIndex <- createDataPartition(diagnosis, p=0.5, list = FALSE)
training <- adData[-testIndex,]
testing <- adData[testIndex,]
str(training)
dim(training)
dim(testing)
library(Hmisc)
remove.packages(Hmisc)
install.packages("Hmisc")
install.packages("acepack")
install.packages("acepack")
data("concrete")
set.seed(1000)
inTrain <-createDataPartition(mixtures$CompressiveStrength, p=3/4)[[1]]
training <- mixtures[inTrain,]
testing <- mixtures[-inTrain,]
cols<- colnames(training)
subcols <- cols[-length(cols)]
plotCols <- 2
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
IL_Colnames = grep("^IL", colnames(training), value=TRUE,ignore.case=TRUE)
pcaMod <- preProcess(training[,IL_Colnames], method="pca", thresh=0.9)
pcaMod
IL_Colnames = grep("^IL", colnames(training), value=TRUE,ignore.case=TRUE)
pcaMod <- preProcess(training[,IL_Colnames], method="pca", thresh=0.8)
pcaMod
q()
library(Hmisc)
install.packages("Hmisc")
library(Hmisc)
exit
q()
q()
library(AppliedPredictiveModeling)
library(caret)
suppressMessages(library(caret))
data("segmentationOriginal")
inTrain <- createDataPartition(y=segmentationOriginal$Case, p=0.6, list = FALSE)
training <- segmentationOriginal[inTrain,]
testing  <- segmentationOriginal[-inTrain,]
set.seed(125)
modfit <- train(Class~.,method= "rpart", data = training)
install.packages("rpart")
library(rpart)
modfit <- train(Class~.,method= "rpart", data = training)
library(caret)
library(AppliedPredictiveModeling)
data("segmentationOriginal")
modfit <- train(Class~.,method= "rpart", data = training)
install.packages("e1071", dependencies = TRUE)
library(e1071)
trace(utils:::unpackPkgZip,edit = TRUE)
install.packages("e1071", dependencies = TRUE)
install.packages("Hmisc")
library(Hmisc)
modfit <- train(Class~., method= "rpart", data= training)
modfit$finalModel
suppressMessages(library(rattle))
install.packages(rattle)
install.packages(Rattle)
install.packages(rattle)
library(rpart.plot)
install.packages("rpart.plot")
library(rpart.plot)
fancyRpartPlot(modfit$finalModel)
install.packages("rattle")
library(rattle)
library("rattle")
library("rattle", dependencies = TRUE)
install.packages("rattle", dependencies = TRUE)
library(rattle)
install.packages("RGtk2", dependencies = TRUE)
?RGtk2
??RGtk2
modfit$finalModel
library(pgmm)
install.packages("pgmm", dependencies = TRUE)
library(pgmm)
data(olive)
olive <- olive[,-1]
dim(olive)
head(olive)
newdata = as.data.frame(t(colMeans(olive)))
newdata
modolive <- train(Area~., method= "rpart", data = olive)
predict(modOlive, newdata)
predict(modolive, newdata)
library(ElemStatLearn)
install.packages("ElemStatLearn")
library(ElemStatLearn)
data("SAheart")
set.seed(8484)
train <- sample(1:dim(SAheart)[1], size= dim(SAheart)[1]/2, replace = F)
trainSA <- SAheart[train,]
testSA <- SAheart[-train,]
missClass = function(values, prediction){sum(((prediction > 0.5) * 1) != values) / length(values)}
set.seed(13234)
modelSA <- train(chd~age+alcohol+obesity+tobacco+typea+ldl, data=trainSA, method = "glm", family = "binomial")
missClass(testSA$chd, predict(modelSA, newdata= testSA))
missClass(trainSA$chd, predict(modelSA, newdata= trainSA))
library(ElemStatLearn)
data("vowel.train")
data("vowel.test")
vowel.train$y = as.factor(vowel.train$y)
vowel.test$y = as.factor(vowel.test$y)
set.seed(33833)
library(randomForest)
modvowel <- randomForest(y ~., data= vowel.train)
order(varImp(modvowel), decreasing = T)
q()
library(quantmod)
library(quantdl)
library(quandl)
install.packages("quandl", dependencies = TRUE)
install.packages("Quandl", dependencies = TRUE)
library(knitr)
library(caret)
llibrary(rpart)
library(rpart)
library(rpart.plot)
library(rattle)
install.packages("RGtk2")
library(randomForest)
library(corrplot)
rm(list = ls())
q()
dir()
q()
setwd("./PML_CourseProject")
##clear the memory
rm(list = ls())
## load libraries needed for this project
library(caret)
library(rpart)
library(rpart.plot)
library(corrplot)
##library(rattle)
library(randomForest)
## Set the seed for reproducibility
set.seed(34324)
## Download the Datasets
##download.file(url ="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", destfile = "pmltraining.csv")
##download.file(url= "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", destfile = "pmltesting.csv")
training <- read.csv(file = "pmltraining.csv")
testing <- read.csv(file = "pmltesting.csv")
inTrain <- createDataPartition(training$classe, p= 0.7, list = FALSE)
trainSet <- training[inTrain,]
testSet <- training[-inTrain,]
dim(trainSet)
dim(testSet)
## remove variables with Near Zero Variance
nzv <- nearZeroVar(trainSet)
trainSet <- trainSet[,-nzv]
testSet <- testSet[,-nzv]
# remove variables that are mostly NA
AllNA    <- sapply(trainSet, function(x) mean(is.na(x))) > 0.95
trainSet <- trainSet[, AllNA==FALSE]
testSet  <- testSet[, AllNA==FALSE]
# remove identification only variables (columns 1 to 5)
trainSet <- trainSet[, -(1:5)]
testSet  <- testSet[, -(1:5)]
dim(trainSet)
dim(testSet)
cMatrix <- cor(trainSet[,-54])
corrplot(cMatrix, method = "color", order= "FPC", type = "lower", tl.col = rgb(1,1,1), tl.cex = 0.7)
## model fit
cRF <- trainControl(method = "cv", number = 3, verboseIter = FALSE)
modFitRF <- train(classe ~ ., data = trainSet, method= "rf", trControl = cRF)
modFitRF$finalModel
##prediction on Test Data Set
##predRF <- predict(modFitRF,newdata=testSet)
##confMatRF <- confusionMatrix(predRF, testSet$classe)
##confMatRF
##Plot the Matrix outputs
## model fit
##cRF <- trainControl(method = "cv", number = 3, verboseIter = FALSE)
##modFitRF <- train(classe ~ ., data = trainSet, method= "rf", trControl = cRF)
##modFitRF$finalModel
##prediction on Test Data Set
predRF <- predict(modFitRF,newdata=testSet)
confMatRF <- confusionMatrix(predRF, testSet$classe)
confMatRF
##Plot the Matrix outputs
## model fit
##cRF <- trainControl(method = "cv", number = 3, verboseIter = FALSE)
##modFitRF <- train(classe ~ ., data = trainSet, method= "rf", trControl = cRF)
##modFitRF$finalModel
##prediction on Test Data Set
predRF <- predict(modFitRF,newdata=testSet)
confMatRF <- confusionMatrix(predRF, testSet$classe)
confMatRF
##Plot the Matrix outputs
plot(confMatRF$table, col=confMatRF$byClass, main=paste("Random Forest - Accuracy =", round(confMatRF$overall['Accuracy'],4)))
## model fit
##cRF <- trainControl(method = "cv", number = 3, verboseIter = FALSE)
##modFitRF <- train(classe ~ ., data = trainSet, method= "rf", trControl = cRF)
##modFitRF$finalModel
##prediction on Test Data Set
predRF <- predict(modFitRF,newdata=testSet)
confMatRF <- confusionMatrix(predRF, testSet$classe)
confMatRF
##Plot the Matrix outputs
plot(confMatRF$table, col=confMatRF$byClass, main=paste("Random Forest - Accuracy =", round(confMatRF$overall['Accuracy'],4)))
## Out of Sample Error Estimate
accRF <- postResample(testSet$classe, predRF)
acvRF <- accRF[[1]]
ofseEst <- 1 - acvRF
ofseEst
## model fit
##cRF <- trainControl(method = "cv", number = 3, verboseIter = FALSE)
##modFitRF <- train(classe ~ ., data = trainSet, method= "rf", trControl = cRF)
##modFitRF$finalModel
##prediction on Test Data Set
predRF <- predict(modFitRF,newdata=testSet)
confMatRF <- confusionMatrix(predRF, testSet$classe)
confMatRF
##Plot the Matrix outputs
plot(confMatRF$table, col=confMatRF$byClass, main=paste("Random Forest - Accuracy =", round(confMatRF$overall['Accuracy'],4)))
## Out of Sample Error Estimate
accRF <- postResample(testSet$classe, predRF)
acvRF <- accRF[[1]]
ofseEst <- 1 - acvRF
ofseEst
set.seed(34324)
## model fit
modFitDT <- rpart(Classe ~ ., method = "class", data = trainSet)
set.seed(34324)
## model fit
modFitDT <- rpart(classe ~ ., method = "class", data = trainSet)
fancyRpartPlot(modFitDT)
set.seed(34324)
## model fit
modFitDT <- rpart(classe ~ ., method = "class", data = trainSet)
##prediction on Test Data Set
predDT <- predict(modFitDT, newdata = testSet, type = "class")
confMatDT <- confusionMatrix(predDT, testSet$classe)
confMatDT
##Plot the Matrix outputs
## Out of Sample Error Estimate
set.seed(34324)
## model fit
modFitDT <- rpart(classe ~ ., method = "class", data = trainSet)
##prediction on Test Data Set
predDT <- predict(modFitDT, newdata = testSet, type = "class")
confMatDT <- confusionMatrix(predDT, testSet$classe)
confMatDT
##Plot the Matrix outputs
plot(confMatDT$table, col=confMatDT$byClass, main = paste("Decision Tree - Accuracy =", round(confMatDT$overall['Accuracy'],4)) )
## Out of Sample Error Estimate
accDT <- postResample(testSet$classe, predDT)
acvDT <- accDT[[1]]
ofseEstDT <- 1 - acvDT
ofseEstDT
set.seed(34324)
## model fit
modFitDT <- rpart(classe ~ ., method = "class", data = trainSet)
##prediction on Test Data Set
predDT <- predict(modFitDT, newdata = testSet, type = "class")
confMatDT <- confusionMatrix(predDT, testSet$classe)
confMatDT
##Plot the Matrix outputs
plot(confMatDT$table, col=confMatDT$byClass, main = paste("Decision Tree - Accuracy =", round(confMatDT$overall['Accuracy'],4)) )
## Out of Sample Error Estimate
accDT <- postResample(testSet$classe, predDT)
acvDT <- accDT[[1]]
ofseEstDT <- 1 - acvDT
ofseEstDT
set.seed(34324)
## model fit
cGBM <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
modFitGBM <- train(classe ~ ., data = trainSet, method = "gbm", verbose = FALSE, trControl = cGBM)
modFitGBM$finalModel
##prediction on Test Data Set
##Plot the Matrix outputs
## Out of Sample Error Estimate
set.seed(34324)
## model fit
##cGBM <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
##modFitGBM <- train(classe ~ ., data = trainSet, method = "gbm", verbose = FALSE, trControl = cGBM)
##modFitGBM$finalModel
##prediction on Test Data Set
predGBM <- predict(modFitGBM, newdata = testSet)
confMatGBM <- confusionMatrix(predGBM, testSet$classe)
confMatGBM
##Plot the Matrix outputs
## Out of Sample Error Estimate
set.seed(34324)
## model fit
##cGBM <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
##modFitGBM <- train(classe ~ ., data = trainSet, method = "gbm", verbose = FALSE, trControl = cGBM)
##modFitGBM$finalModel
##prediction on Test Data Set
predGBM <- predict(modFitGBM, newdata = testSet)
confMatGBM <- confusionMatrix(predGBM, testSet$classe)
confMatGBM
##Plot the Matrix outputs
plot(confMatGBM$table, col= confMatGBM$byClass, main = paste("GBM - Accuracy = ", round(confMatGBM$overall['Accuracy'],4)))
## Out of Sample Error Estimate
accGBM <- postResample(testSet$classe, predGBM)
acvGBM <- accGBM[[1]]
ofseEstGBM <- 1 - acvGBM
ofseEstGBM
predFinal <- predict(modFitRF, newdata = testing)
predFinal
q()
